{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\20193709\\AppData\\Local\\anaconda3\\envs\\ml4sc\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.filters import threshold_otsu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_default_device():\n",
    "    \"\"\"Pick GPU if available, else CPU\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device('cuda')\n",
    "    else:\n",
    "        return torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = get_default_device()\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the U-Net architecture for binary segmentation\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(UNet, self).__init__()\n",
    "\n",
    "        # Encoder (downsampling path)\n",
    "        self.enc_conv1 = nn.Conv2d(3, 64, kernel_size=3, padding=1)\n",
    "        self.enc_conv2 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
    "        self.enc_pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.enc_conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.enc_conv4 = nn.Conv2d(128, 128, kernel_size=3, padding=1)\n",
    "        self.enc_pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.enc_conv5 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
    "        self.enc_conv6 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n",
    "        self.enc_pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.enc_conv7 = nn.Conv2d(256, 512, kernel_size=3, padding=1)\n",
    "        self.enc_conv8 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "        self.enc_pool4 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        # Bottleneck\n",
    "        self.bottleneck_conv1 = nn.Conv2d(512, 1024, kernel_size=3, padding=1)\n",
    "        self.bottleneck_conv2 = nn.Conv2d(1024, 1024, kernel_size=3, padding=1)\n",
    "\n",
    "        # Decoder (upsampling path)\n",
    "        self.dec_upsample1 = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False)\n",
    "        self.dec_conv1 = nn.Conv2d(1024 + 512, 512, kernel_size=3, padding=1)\n",
    "        self.dec_conv2 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "\n",
    "        self.dec_upsample2 = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False)\n",
    "        self.dec_conv3 = nn.Conv2d(512 + 256, 256, kernel_size=3, padding=1)\n",
    "        self.dec_conv4 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n",
    "\n",
    "        self.dec_upsample3 = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False)\n",
    "        self.dec_conv5 = nn.Conv2d(256 + 128, 128, kernel_size=3, padding=1)\n",
    "        self.dec_conv6 = nn.Conv2d(128, 128, kernel_size=3, padding=1)\n",
    "\n",
    "        self.dec_upsample4 = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False)\n",
    "        self.dec_conv7 = nn.Conv2d(128 + 64, 64, kernel_size=3, padding=1)\n",
    "        self.dec_conv8 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
    "\n",
    "          # Output\n",
    "        self.output_conv = nn.Conv2d(64, 1, kernel_size=1)  # 1x1 convolution for binary segmentation\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder (downsampling path)\n",
    "        enc1 = nn.ReLU()(self.enc_conv1(x))\n",
    "        enc2 = nn.ReLU()(self.enc_conv2(enc1))\n",
    "        enc2_pool = self.enc_pool1(enc2)\n",
    "\n",
    "        enc3 = nn.ReLU()(self.enc_conv3(enc2_pool))\n",
    "        enc4 = nn.ReLU()(self.enc_conv4(enc3))\n",
    "        enc4_pool = self.enc_pool2(enc4)\n",
    "\n",
    "        enc5 = nn.ReLU()(self.enc_conv5(enc4_pool))\n",
    "        enc6 = nn.ReLU()(self.enc_conv6(enc5))\n",
    "        enc6_pool = self.enc_pool3(enc6)\n",
    "\n",
    "        enc7 = nn.ReLU()(self.enc_conv7(enc6_pool))\n",
    "        enc8 = nn.ReLU()(self.enc_conv8(enc7))\n",
    "        enc8_pool = self.enc_pool4(enc8)\n",
    "\n",
    "        # Bottleneck\n",
    "        bottleneck = nn.ReLU()(self.bottleneck_conv1(enc8_pool))\n",
    "        bottleneck = nn.ReLU()(self.bottleneck_conv2(bottleneck))\n",
    "\n",
    "        # Decoder (upsampling path)\n",
    "        dec1 = self.dec_upsample1(bottleneck)\n",
    "        dec1 = torch.cat([dec1, enc8], dim=1)\n",
    "        dec1 = nn.ReLU()(self.dec_conv1(dec1))\n",
    "        dec1 = nn.ReLU()(self.dec_conv2(dec1))\n",
    "\n",
    "        dec2 = self.dec_upsample2(dec1)\n",
    "        dec2 = torch.cat([dec2, enc6], dim=1)\n",
    "        dec2 = nn.ReLU()(self.dec_conv3(dec2))\n",
    "        dec2 = nn.ReLU()(self.dec_conv4(dec2))\n",
    "\n",
    "        dec3 = self.dec_upsample3(dec2)\n",
    "        dec3 = torch.cat([dec3, enc4], dim=1)\n",
    "        dec3 = nn.ReLU()(self.dec_conv5(dec3))\n",
    "        dec3 = nn.ReLU()(self.dec_conv6(dec3))\n",
    "\n",
    "        dec4 = self.dec_upsample4(dec3)\n",
    "        dec4 = torch.cat([dec4, enc2], dim=1)\n",
    "        dec4 = nn.ReLU()(self.dec_conv7(dec4))\n",
    "        dec4 = nn.ReLU()(self.dec_conv8(dec4))\n",
    "\n",
    "        # Output\n",
    "        output = self.output_conv(dec4)\n",
    "        return output   \n",
    "    \n",
    "    # Create an instance of the FCN model\n",
    "model = UNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define criterion and optimizer\n",
    "criterion = nn.MSELoss()  # nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transforms\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    # transforms.Normalize(mean=[0.4414, 0.4456, 0.3421],  # Normalization of real images\n",
    "    #                         std=[0.2741, 0.2734, 0.3030])\n",
    "])\n",
    "transform_mask = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this example, the CustomDataset takes in two lists of file paths: image_list and mask_list, representing the paths to the images and their corresponding masks, \n",
    "# respectively. It also takes in an optional transform argument to apply any necessary transformations to the images and masks.\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, image_list, mask_list, transform=None, transform_mask=None):\n",
    "        # self.image_list = image_list\n",
    "        # self.mask_list = mask_list\n",
    "        self.image_list = [os.path.join(image_list, f) for f in os.listdir(image_list)]\n",
    "        self.mask_list = [os.path.join(mask_list, f) for f in os.listdir(mask_list)]\n",
    "        self.transform = transform\n",
    "        self.transform_mask = transform_mask\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_list)\n",
    "    \n",
    "    # def binarize_mask(self, mask):\n",
    "    #     threshold_value = threshold_otsu(mask)\n",
    "    #     binary_mask = (mask >= threshold_value).astype(int)\n",
    "    #     return binary_mask\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.image_list[idx]\n",
    "        mask_path = self.mask_list[idx]\n",
    "        image = Image.open(image_path)\n",
    "        image = image.convert('RGB')\n",
    "        mask = Image.open(mask_path)\n",
    "        mask = mask.convert('L')\n",
    "        # threshold_value = threshold_otsu(mask)\n",
    "        # threshold_value = 0.0040  # Adjust the threshold value as needed\n",
    "        # mask = (np.array(mask) >= threshold_value).astype(np.uint8)\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        if self.transform_mask:\n",
    "            mask = self.transform_mask(mask)\n",
    "\n",
    "\n",
    "        return image, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = CustomDataset('17_06/102_Real/','17_06/102_Mask/', transform=transform, transform_mask=transform_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, val_data = train_test_split(data, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the data loader\n",
    "batch_size = 8\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "len_train = len(train_loader)*batch_size\n",
    "len_val = len(val_loader)*batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: tensor([0.4084, 0.4125, 0.3170])\n",
      "std:  tensor([0.2865, 0.2865, 0.3031])\n"
     ]
    }
   ],
   "source": [
    "# Mean and STD calculations for real images\n",
    "psum    = torch.tensor([0.0, 0.0, 0.0])\n",
    "psum_sq = torch.tensor([0.0, 0.0, 0.0])\n",
    "\n",
    "# loop through images\n",
    "# for inputs in train_data:#DataLoader(train_data, batch_size=batch_size, shuffle=True):\n",
    "for i, (images, labels) in enumerate(train_loader):\n",
    "    psum    += images.sum(axis        = [0, 2, 3])\n",
    "    psum_sq += (images ** 2).sum(axis = [0, 2, 3])\n",
    "    \n",
    "\n",
    "# Final Calculation\n",
    "# pixel count\n",
    "image_size = 256\n",
    "count = len_train * image_size * image_size\n",
    "\n",
    "# mean and std\n",
    "total_mean = psum / count\n",
    "total_var  = (psum_sq / count) - (total_mean ** 2)\n",
    "total_std  = torch.sqrt(total_var)\n",
    "\n",
    "# output\n",
    "print('mean: '  + str(total_mean))\n",
    "print('std:  '  + str(total_std))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: 0.1293376386165619\n",
      "Standard Deviation: 0.3355732858181\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "tensor_list = []\n",
    "\n",
    "for i, (images, labels) in enumerate(train_loader):\n",
    "    threshold_value = threshold_otsu(labels.squeeze(0).squeeze(0).cpu().numpy())\n",
    "    binary_mask = (np.array(labels) >= threshold_value).astype(np.float32)\n",
    "    binary_mask = torch.from_numpy(binary_mask)\n",
    "    \n",
    "    tensor_list.append(binary_mask)\n",
    "\n",
    "# Concatenate the tensors in the list to form one big tensor\n",
    "big_tensor = torch.cat(tensor_list)\n",
    "\n",
    "# Compute the mean and standard deviation from the whole tensor\n",
    "total_mean_mask = torch.mean(big_tensor.float())\n",
    "total_std_mask = torch.std(big_tensor.float())\n",
    "\n",
    "print(\"Mean:\", total_mean_mask.item())\n",
    "print(\"Standard Deviation:\", total_std_mask.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Redefine Transforms\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=total_mean,  # Normalization of real images\n",
    "                            std=total_std)\n",
    "])\n",
    "\n",
    "transform_mask = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    # transforms.Normalize(mean=total_mean_mask,  # Normalization of masks\n",
    "    #                         std=total_std_mask)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = CustomDataset('15_06/102_Real/','15_06/102_Mask/', transform=transform, transform_mask=transform_mask)\n",
    "train_data, val_data = train_test_split(data, test_size=0.2, random_state=42)\n",
    "\n",
    "batch_size = 8\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x236be5e03a0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Check that data is truly binarized\n",
    "images, labels = data[0]\n",
    "threshold_value = threshold_otsu(labels.squeeze(0).squeeze(0).cpu().numpy())\n",
    "binary_mask = (np.array(labels) >= threshold_value).astype(np.float32)\n",
    "binary_mask = torch.from_numpy(binary_mask)\n",
    "plt.imshow(binary_mask.squeeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Batch [1/11], Loss: 0.1232\n",
      "Epoch [1/10], Batch [2/11], Loss: 0.1146\n",
      "Epoch [1/10], Batch [3/11], Loss: 249.6318\n",
      "Epoch [1/10], Batch [4/11], Loss: 0.1131\n",
      "Epoch [1/10], Batch [5/11], Loss: 0.1161\n",
      "Epoch [1/10], Batch [6/11], Loss: 0.1171\n",
      "Epoch [1/10], Batch [7/11], Loss: 0.1174\n",
      "Epoch [1/10], Batch [8/11], Loss: 0.1178\n",
      "Epoch [1/10], Batch [9/11], Loss: 0.1174\n",
      "Epoch [1/10], Batch [10/11], Loss: 0.1181\n",
      "Epoch [1/10], Batch [11/11], Loss: 0.1159\n",
      "Epoch [1/10] , Val Loss: 0.1163\n",
      "Epoch [2/10], Batch [1/11], Loss: 0.1160\n",
      "Epoch [2/10], Batch [2/11], Loss: 0.1145\n",
      "Epoch [2/10], Batch [3/11], Loss: 0.1229\n",
      "Epoch [2/10], Batch [4/11], Loss: 0.1134\n",
      "Epoch [2/10], Batch [5/11], Loss: 0.1143\n",
      "Epoch [2/10], Batch [6/11], Loss: 0.1158\n",
      "Epoch [2/10], Batch [7/11], Loss: 0.1162\n",
      "Epoch [2/10], Batch [8/11], Loss: 0.1168\n",
      "Epoch [2/10], Batch [9/11], Loss: 0.1160\n",
      "Epoch [2/10], Batch [10/11], Loss: 0.1161\n",
      "Epoch [2/10], Batch [11/11], Loss: 0.1147\n",
      "Epoch [2/10] , Val Loss: 0.1157\n",
      "Epoch [3/10], Batch [1/11], Loss: 0.1158\n",
      "Epoch [3/10], Batch [2/11], Loss: 0.1152\n",
      "Epoch [3/10], Batch [3/11], Loss: 0.1147\n",
      "Epoch [3/10], Batch [4/11], Loss: 0.1139\n",
      "Epoch [3/10], Batch [5/11], Loss: 0.1133\n",
      "Epoch [3/10], Batch [6/11], Loss: 0.1133\n",
      "Epoch [3/10], Batch [7/11], Loss: 0.1136\n",
      "Epoch [3/10], Batch [8/11], Loss: 0.1130\n",
      "Epoch [3/10], Batch [9/11], Loss: 0.1131\n",
      "Epoch [3/10], Batch [10/11], Loss: 0.1124\n",
      "Epoch [3/10], Batch [11/11], Loss: 0.1109\n",
      "Epoch [3/10] , Val Loss: 0.1119\n",
      "Epoch [4/10], Batch [1/11], Loss: 0.1121\n",
      "Epoch [4/10], Batch [2/11], Loss: 0.1130\n",
      "Epoch [4/10], Batch [3/11], Loss: 0.1107\n",
      "Epoch [4/10], Batch [4/11], Loss: 0.1095\n",
      "Epoch [4/10], Batch [5/11], Loss: 0.1108\n",
      "Epoch [4/10], Batch [6/11], Loss: 0.1129\n",
      "Epoch [4/10], Batch [7/11], Loss: 0.1128\n",
      "Epoch [4/10], Batch [8/11], Loss: 0.1115\n",
      "Epoch [4/10], Batch [9/11], Loss: 0.1101\n",
      "Epoch [4/10], Batch [10/11], Loss: 0.1112\n",
      "Epoch [4/10], Batch [11/11], Loss: 0.1110\n",
      "Epoch [4/10] , Val Loss: 0.1101\n",
      "Epoch [5/10], Batch [1/11], Loss: 0.1098\n",
      "Epoch [5/10], Batch [2/11], Loss: 0.1100\n",
      "Epoch [5/10], Batch [3/11], Loss: 0.1111\n",
      "Epoch [5/10], Batch [4/11], Loss: 0.1106\n",
      "Epoch [5/10], Batch [5/11], Loss: 0.1094\n",
      "Epoch [5/10], Batch [6/11], Loss: 0.1090\n",
      "Epoch [5/10], Batch [7/11], Loss: 0.1094\n",
      "Epoch [5/10], Batch [8/11], Loss: 0.1094\n",
      "Epoch [5/10], Batch [9/11], Loss: 0.1082\n",
      "Epoch [5/10], Batch [10/11], Loss: 0.1084\n",
      "Epoch [5/10], Batch [11/11], Loss: 0.1066\n",
      "Epoch [5/10] , Val Loss: 0.1075\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 16\u001b[0m\n\u001b[0;32m     13\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m     15\u001b[0m \u001b[39m# Forward pass\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m outputs \u001b[39m=\u001b[39m model(images)\n\u001b[0;32m     18\u001b[0m threshold_value \u001b[39m=\u001b[39m threshold_otsu(labels\u001b[39m.\u001b[39msqueeze(\u001b[39m0\u001b[39m)\u001b[39m.\u001b[39msqueeze(\u001b[39m0\u001b[39m)\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mnumpy())\n\u001b[0;32m     19\u001b[0m binary_mask \u001b[39m=\u001b[39m (np\u001b[39m.\u001b[39marray(labels) \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m threshold_value)\u001b[39m.\u001b[39mastype(np\u001b[39m.\u001b[39mfloat32)\n",
      "File \u001b[1;32mc:\\Users\\20193709\\AppData\\Local\\anaconda3\\envs\\ml4sc\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[4], line 68\u001b[0m, in \u001b[0;36mUNet.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[39m# Bottleneck\u001b[39;00m\n\u001b[0;32m     67\u001b[0m bottleneck \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mReLU()(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbottleneck_conv1(enc8_pool))\n\u001b[1;32m---> 68\u001b[0m bottleneck \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mReLU()(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbottleneck_conv2(bottleneck))\n\u001b[0;32m     70\u001b[0m \u001b[39m# Decoder (upsampling path)\u001b[39;00m\n\u001b[0;32m     71\u001b[0m dec1 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdec_upsample1(bottleneck)\n",
      "File \u001b[1;32mc:\\Users\\20193709\\AppData\\Local\\anaconda3\\envs\\ml4sc\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\20193709\\AppData\\Local\\anaconda3\\envs\\ml4sc\\lib\\site-packages\\torch\\nn\\modules\\conv.py:457\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    456\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 457\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[1;32mc:\\Users\\20193709\\AppData\\Local\\anaconda3\\envs\\ml4sc\\lib\\site-packages\\torch\\nn\\modules\\conv.py:453\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    449\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m    450\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[0;32m    451\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[0;32m    452\u001b[0m                     _pair(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[1;32m--> 453\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv2d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[0;32m    454\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "train_loss_list = []\n",
    "valid_loss_list = []\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = 0.0\n",
    "    valid_loss = 0.0\n",
    "    # Set the model to training mode\n",
    "    model.train()\n",
    "    \n",
    "    # Iterate over the training data\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "\n",
    "        threshold_value = threshold_otsu(labels.squeeze(0).squeeze(0).cpu().numpy())\n",
    "        binary_mask = (np.array(labels) >= threshold_value).astype(np.float32)\n",
    "        binary_mask = torch.from_numpy(binary_mask)\n",
    "        \n",
    "        # Compute the loss\n",
    "        loss = criterion(outputs, binary_mask)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update the parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()*images.size(0)\n",
    "        # Print the loss every 10 batches\n",
    "        if i % 1 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Batch [{i+1}/{len(train_loader)}], Loss: {loss.item():.4f}')\n",
    "    \n",
    "    # Set the model to evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    # Compute the validation loss\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "\n",
    "            threshold_value = threshold_otsu(labels.squeeze(0).squeeze(0).cpu().numpy())\n",
    "            binary_mask = (np.array(labels) >= threshold_value).astype(np.float32)\n",
    "            binary_mask = torch.from_numpy(binary_mask)\n",
    "            # Compute the loss\n",
    "            loss = criterion(outputs, binary_mask)\n",
    "            \n",
    "            valid_loss += loss.item()*images.size(0)\n",
    "            # Accumulate the loss over all batches\n",
    "            val_loss += loss.item() * images.size(0)\n",
    "    # Compute the average validation loss\n",
    "    val_loss /= len(val_data)\n",
    "      # calculate average losses\n",
    "    train_loss = train_loss/len(train_loader.dataset)\n",
    "    valid_loss = valid_loss/len(val_loader.dataset)\n",
    "    train_loss_list.append(train_loss)\n",
    "    valid_loss_list.append(valid_loss)\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}] , Val Loss: {val_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = {'Train_loss': [train_loss_list],\n",
    "            'Valid_Loss': [valid_loss_list]}\n",
    "\n",
    "df = pd.DataFrame(raw_data, columns = ['Train_loss', 'Valid_Loss'])\n",
    "# Always check/change name for new model\n",
    "df.to_csv('Losses_binary_102.csv', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(10,10))\n",
    "plt.plot(train_loss_list,  marker='o', label=\"Training Loss\")\n",
    "plt.plot(valid_loss_list,  marker='o', label=\"Validation Loss\")\n",
    "plt.ylabel('loss')\n",
    "plt.yscale('log')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'Unet_16_06_binary_test.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "model = UNet()\n",
    "model.load_state_dict(torch.load('Unet_13_06_512Test.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing model on given outptut\n",
    "# image_path = \"Droplet_XY_val/drop_s40_v5_r0.5_str2_pos2.png\"drop_s40_v5_r0.5_str5_pos11.png\n",
    "image_path = \"13_06/512_Real/drop_s32_v5_r0.5_str6_pos0_cam1.png\"\n",
    "# image_path = \"syringe-needle-with-droplet-close-up-stuart-minzey.jpg\"\n",
    "image = Image.open(image_path)\n",
    "image = image.convert('RGB')\n",
    "# image = image.resize((256,256))\n",
    "img = transform(image)\n",
    "img = img.unsqueeze(0)\n",
    "img.size()\n",
    "\n",
    "image_mask_path = \"13_06/512_Mask/drop_s32_v5_r0.5_str6_pos0_cam1.png\"\n",
    "image_mask = Image.open(image_mask_path)\n",
    "image_mask = image_mask.convert('RGB')\n",
    "img_mask = transform_mask(image_mask)\n",
    "img_mask = img_mask.unsqueeze(0)\n",
    "img_mask.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming you have the trained model and a test image\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Perform forward pass on the test image\n",
    "with torch.no_grad():\n",
    "    output = model(img)  # Assuming you have the test image as a tensor\n",
    "\n",
    "# Convert the output tensor to a numpy array and squeeze the batch and channel dimensions\n",
    "output_np = output.squeeze(0).squeeze(0).cpu().numpy()*255\n",
    "\n",
    "# Apply a threshold to convert the output to a binary mask\n",
    "# threshold = 0.5  # Adjust the threshold value as needed\n",
    "mask = (output_np.astype(np.uint8)).T\n",
    "# mask = mask[0,:,:]\n",
    "# Plot the original test image\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.imshow(image)\n",
    "plt.title('Original Image')\n",
    "# Plot the original mask\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.imshow(image_mask)\n",
    "plt.title('Original Mask')\n",
    "# Plot the generated mask\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.imshow(mask)\n",
    "plt.title('Generated Mask')\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml4sc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
