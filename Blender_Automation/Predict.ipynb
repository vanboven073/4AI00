{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from torchvision.transforms import Normalize\n",
    "from torchvision.transforms import ToTensor\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CNN and UNET\n",
    "from CNN_Mask_to_Tens import CNN\n",
    "# from UNet import UNet\n",
    "# Define the U-Net architecture for binary segmentation\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(UNet, self).__init__()\n",
    "\n",
    "        # Encoder (downsampling path)\n",
    "        self.enc_conv1 = nn.Conv2d(3, 64, kernel_size=3, padding=1)\n",
    "        self.enc_conv2 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
    "        self.enc_pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.enc_conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.enc_conv4 = nn.Conv2d(128, 128, kernel_size=3, padding=1)\n",
    "        self.enc_pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.enc_conv5 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
    "        self.enc_conv6 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n",
    "        self.enc_pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.enc_conv7 = nn.Conv2d(256, 512, kernel_size=3, padding=1)\n",
    "        self.enc_conv8 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "        self.enc_pool4 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        # Bottleneck\n",
    "        self.bottleneck_conv1 = nn.Conv2d(512, 1024, kernel_size=3, padding=1)\n",
    "        self.bottleneck_conv2 = nn.Conv2d(1024, 1024, kernel_size=3, padding=1)\n",
    "\n",
    "        # Decoder (upsampling path)\n",
    "        self.dec_upsample1 = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False)\n",
    "        self.dec_conv1 = nn.Conv2d(1024 + 512, 512, kernel_size=3, padding=1)\n",
    "        self.dec_conv2 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "\n",
    "        self.dec_upsample2 = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False)\n",
    "        self.dec_conv3 = nn.Conv2d(512 + 256, 256, kernel_size=3, padding=1)\n",
    "        self.dec_conv4 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n",
    "\n",
    "        self.dec_upsample3 = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False)\n",
    "        self.dec_conv5 = nn.Conv2d(256 + 128, 128, kernel_size=3, padding=1)\n",
    "        self.dec_conv6 = nn.Conv2d(128, 128, kernel_size=3, padding=1)\n",
    "\n",
    "        self.dec_upsample4 = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False)\n",
    "        self.dec_conv7 = nn.Conv2d(128 + 64, 64, kernel_size=3, padding=1)\n",
    "        self.dec_conv8 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
    "\n",
    "          # Output\n",
    "        self.output_conv = nn.Conv2d(64, 1, kernel_size=1)  # 1x1 convolution for binary segmentation\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder (downsampling path)\n",
    "        enc1 = nn.ReLU()(self.enc_conv1(x))\n",
    "        enc2 = nn.ReLU()(self.enc_conv2(enc1))\n",
    "        enc2_pool = self.enc_pool1(enc2)\n",
    "\n",
    "        enc3 = nn.ReLU()(self.enc_conv3(enc2_pool))\n",
    "        enc4 = nn.ReLU()(self.enc_conv4(enc3))\n",
    "        enc4_pool = self.enc_pool2(enc4)\n",
    "\n",
    "        enc5 = nn.ReLU()(self.enc_conv5(enc4_pool))\n",
    "        enc6 = nn.ReLU()(self.enc_conv6(enc5))\n",
    "        enc6_pool = self.enc_pool3(enc6)\n",
    "\n",
    "        enc7 = nn.ReLU()(self.enc_conv7(enc6_pool))\n",
    "        enc8 = nn.ReLU()(self.enc_conv8(enc7))\n",
    "        enc8_pool = self.enc_pool4(enc8)\n",
    "\n",
    "        # Bottleneck\n",
    "        bottleneck = nn.ReLU()(self.bottleneck_conv1(enc8_pool))\n",
    "        bottleneck = nn.ReLU()(self.bottleneck_conv2(bottleneck))\n",
    "\n",
    "        # Decoder (upsampling path)\n",
    "        dec1 = self.dec_upsample1(bottleneck)\n",
    "        dec1 = torch.cat([dec1, enc8], dim=1)\n",
    "        dec1 = nn.ReLU()(self.dec_conv1(dec1))\n",
    "        dec1 = nn.ReLU()(self.dec_conv2(dec1))\n",
    "\n",
    "        dec2 = self.dec_upsample2(dec1)\n",
    "        dec2 = torch.cat([dec2, enc6], dim=1)\n",
    "        dec2 = nn.ReLU()(self.dec_conv3(dec2))\n",
    "        dec2 = nn.ReLU()(self.dec_conv4(dec2))\n",
    "\n",
    "        dec3 = self.dec_upsample3(dec2)\n",
    "        dec3 = torch.cat([dec3, enc4], dim=1)\n",
    "        dec3 = nn.ReLU()(self.dec_conv5(dec3))\n",
    "        dec3 = nn.ReLU()(self.dec_conv6(dec3))\n",
    "\n",
    "        dec4 = self.dec_upsample4(dec3)\n",
    "        dec4 = torch.cat([dec4, enc2], dim=1)\n",
    "        dec4 = nn.ReLU()(self.dec_conv7(dec4))\n",
    "        dec4 = nn.ReLU()(self.dec_conv8(dec4))\n",
    "\n",
    "        # Output\n",
    "        output = self.output_conv(dec4)\n",
    "        return output   \n",
    "    \n",
    "    # Create an instance of the FCN model\n",
    "model = UNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess the input image\n",
    "image = Image.open('Test_Images\\Train_8160\\drop_s30_v5_r0.5_str6_pos0_cam2.png')  # Replace 'input_image.jpg' with the path to your input image file\n",
    "\n",
    "# Create transform\n",
    "transform = transforms.Compose([\n",
    "    # transforms.Resize((256, 256)),  # Resize the image to a specific size\n",
    "    transforms.ToTensor(),    \n",
    "    transforms.Normalize(mean=[0.4414, 0.4456, 0.3421],  # Normalization of real images\n",
    "                            std=[0.2741, 0.2734, 0.3030])\n",
    "])\n",
    "\n",
    "image = image.convert('RGB')\n",
    "image = transform(image)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Unet_R3080.pt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\domin\\Desktop\\4AI00\\4AI00\\Blender_Automation\\Predict.ipynb Cell 4\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/domin/Desktop/4AI00/4AI00/Blender_Automation/Predict.ipynb#X11sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m model \u001b[39m=\u001b[39m UNet()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/domin/Desktop/4AI00/4AI00/Blender_Automation/Predict.ipynb#X11sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39m# Load the trained weights\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/domin/Desktop/4AI00/4AI00/Blender_Automation/Predict.ipynb#X11sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m model\u001b[39m.\u001b[39mload_state_dict(torch\u001b[39m.\u001b[39;49mload(\u001b[39m'\u001b[39;49m\u001b[39mUnet_R3080.pt\u001b[39;49m\u001b[39m'\u001b[39;49m)) \u001b[39m# Change trained_model to use desired weights\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/domin/Desktop/4AI00/4AI00/Blender_Automation/Predict.ipynb#X11sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39m# Set the model to evaluation mode\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/domin/Desktop/4AI00/4AI00/Blender_Automation/Predict.ipynb#X11sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m model\u001b[39m.\u001b[39meval()\n",
      "File \u001b[1;32mc:\\Users\\domin\\anaconda3\\lib\\site-packages\\torch\\serialization.py:771\u001b[0m, in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, weights_only, **pickle_load_args)\u001b[0m\n\u001b[0;32m    768\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mencoding\u001b[39m\u001b[39m'\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m pickle_load_args\u001b[39m.\u001b[39mkeys():\n\u001b[0;32m    769\u001b[0m     pickle_load_args[\u001b[39m'\u001b[39m\u001b[39mencoding\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m--> 771\u001b[0m \u001b[39mwith\u001b[39;00m _open_file_like(f, \u001b[39m'\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39mas\u001b[39;00m opened_file:\n\u001b[0;32m    772\u001b[0m     \u001b[39mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[0;32m    773\u001b[0m         \u001b[39m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[0;32m    774\u001b[0m         \u001b[39m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[0;32m    775\u001b[0m         \u001b[39m# reset back to the original position.\u001b[39;00m\n\u001b[0;32m    776\u001b[0m         orig_position \u001b[39m=\u001b[39m opened_file\u001b[39m.\u001b[39mtell()\n",
      "File \u001b[1;32mc:\\Users\\domin\\anaconda3\\lib\\site-packages\\torch\\serialization.py:270\u001b[0m, in \u001b[0;36m_open_file_like\u001b[1;34m(name_or_buffer, mode)\u001b[0m\n\u001b[0;32m    268\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[0;32m    269\u001b[0m     \u001b[39mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[1;32m--> 270\u001b[0m         \u001b[39mreturn\u001b[39;00m _open_file(name_or_buffer, mode)\n\u001b[0;32m    271\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    272\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mw\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m mode:\n",
      "File \u001b[1;32mc:\\Users\\domin\\anaconda3\\lib\\site-packages\\torch\\serialization.py:251\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[1;34m(self, name, mode)\u001b[0m\n\u001b[0;32m    250\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, name, mode):\n\u001b[1;32m--> 251\u001b[0m     \u001b[39msuper\u001b[39m(_open_file, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\u001b[39mopen\u001b[39;49m(name, mode))\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Unet_R3080.pt'"
     ]
    }
   ],
   "source": [
    "from skimage.filters import threshold_otsu\n",
    "# UNET PART\n",
    "model = UNet()\n",
    "\n",
    "# Load the trained weights\n",
    "model.load_state_dict(torch.load('Unet_R3080.pt')) # Change trained_model to use desired weights\n",
    "\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "# Make a prediction\n",
    "with torch.no_grad():\n",
    "    output = model(image.unsqueeze(0))  # Assuming you have the test image as a tensor\n",
    "\n",
    "# Convert the output tensor to a numpy array and squeeze the batch and channel dimensions\n",
    "output_1 = output.squeeze(0).squeeze(0).cpu().detach()\n",
    "output_np = output_1.numpy()\n",
    "\n",
    "threshold = 0.5  # Adjust the threshold value as needed\n",
    "# Apply a threshold to convert the output to a binary mask\n",
    "threshold_value = threshold_otsu(output_np)\n",
    "mask = (output_np >= threshold).astype(np.uint8)\n",
    "\n",
    "# Assuming tensor_image is your tensor representation of the image\n",
    "# Convert the tensor to a PIL Image\n",
    "pil_image = transforms.ToPILImage()(mask)\n",
    "\n",
    "pil_image.save(\"output_UNET.jpg\")\n",
    "\n",
    "# Convert the PIL Image to a NumPy array\n",
    "numpy_image = np.array(pil_image)\n",
    "\n",
    "# Display the image using matplotlib\n",
    "plt.imshow(numpy_image)\n",
    "plt.axis('off')  # Optional: Turn off axis labels and ticks\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted value: 28.731781005859375\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize(64),\n",
    "    transforms.CenterCrop(64),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.4753, 0.4753, 0.4753])\n",
    "])\n",
    "\n",
    "# Load model\n",
    "model = CNN()\n",
    "\n",
    "# Load the trained weights\n",
    "model.load_state_dict(torch.load('trained_model_Train_510.pt',map_location=torch.device('cpu'))) # Change trained_model to use desired weights\n",
    "# Set the model in evaluation mode\n",
    "\n",
    "model.eval()\n",
    "\n",
    "input_image = pil_image.convert('RGB')\n",
    "input_tensor = transform(input_image)\n",
    "\n",
    "# Make a prediction\n",
    "with torch.no_grad():\n",
    "    output_CNN = model(input_tensor)\n",
    "\n",
    "# Convert the output to a readable format\n",
    "predicted_value = output_CNN.item()\n",
    "\n",
    "# Print the predicted value\n",
    "print('Predicted value:', predicted_value)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
