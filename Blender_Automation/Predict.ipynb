{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from torchvision.transforms import Normalize\n",
    "from torchvision.transforms import ToTensor\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CNN and UNET\n",
    "from CNN_Mask_to_Tens import CNN\n",
    "from UNet import UNet\n",
    "# from UNet import UNet\n",
    "# # Define the U-Net architecture for binary segmentation\n",
    "# class UNet(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(UNet, self).__init__()\n",
    "\n",
    "#         # Encoder (downsampling path)\n",
    "#         self.enc_conv1 = nn.Conv2d(3, 64, kernel_size=3, padding=1)\n",
    "#         self.enc_conv2 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
    "#         self.enc_pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "#         self.enc_conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "#         self.enc_conv4 = nn.Conv2d(128, 128, kernel_size=3, padding=1)\n",
    "#         self.enc_pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "#         self.enc_conv5 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
    "#         self.enc_conv6 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n",
    "#         self.enc_pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "#         self.enc_conv7 = nn.Conv2d(256, 512, kernel_size=3, padding=1)\n",
    "#         self.enc_conv8 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "#         self.enc_pool4 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "#         # Bottleneck\n",
    "#         self.bottleneck_conv1 = nn.Conv2d(512, 1024, kernel_size=3, padding=1)\n",
    "#         self.bottleneck_conv2 = nn.Conv2d(1024, 1024, kernel_size=3, padding=1)\n",
    "\n",
    "#         # Decoder (upsampling path)\n",
    "#         self.dec_upsample1 = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False)\n",
    "#         self.dec_conv1 = nn.Conv2d(1024 + 512, 512, kernel_size=3, padding=1)\n",
    "#         self.dec_conv2 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "\n",
    "#         self.dec_upsample2 = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False)\n",
    "#         self.dec_conv3 = nn.Conv2d(512 + 256, 256, kernel_size=3, padding=1)\n",
    "#         self.dec_conv4 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n",
    "\n",
    "#         self.dec_upsample3 = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False)\n",
    "#         self.dec_conv5 = nn.Conv2d(256 + 128, 128, kernel_size=3, padding=1)\n",
    "#         self.dec_conv6 = nn.Conv2d(128, 128, kernel_size=3, padding=1)\n",
    "\n",
    "#         self.dec_upsample4 = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False)\n",
    "#         self.dec_conv7 = nn.Conv2d(128 + 64, 64, kernel_size=3, padding=1)\n",
    "#         self.dec_conv8 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
    "\n",
    "#           # Output\n",
    "#         self.output_conv = nn.Conv2d(64, 1, kernel_size=1)  # 1x1 convolution for binary segmentation\n",
    "\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         # Encoder (downsampling path)\n",
    "#         enc1 = nn.ReLU()(self.enc_conv1(x))\n",
    "#         enc2 = nn.ReLU()(self.enc_conv2(enc1))\n",
    "#         enc2_pool = self.enc_pool1(enc2)\n",
    "\n",
    "#         enc3 = nn.ReLU()(self.enc_conv3(enc2_pool))\n",
    "#         enc4 = nn.ReLU()(self.enc_conv4(enc3))\n",
    "#         enc4_pool = self.enc_pool2(enc4)\n",
    "\n",
    "#         enc5 = nn.ReLU()(self.enc_conv5(enc4_pool))\n",
    "#         enc6 = nn.ReLU()(self.enc_conv6(enc5))\n",
    "#         enc6_pool = self.enc_pool3(enc6)\n",
    "\n",
    "#         enc7 = nn.ReLU()(self.enc_conv7(enc6_pool))\n",
    "#         enc8 = nn.ReLU()(self.enc_conv8(enc7))\n",
    "#         enc8_pool = self.enc_pool4(enc8)\n",
    "\n",
    "#         # Bottleneck\n",
    "#         bottleneck = nn.ReLU()(self.bottleneck_conv1(enc8_pool))\n",
    "#         bottleneck = nn.ReLU()(self.bottleneck_conv2(bottleneck))\n",
    "\n",
    "#         # Decoder (upsampling path)\n",
    "#         dec1 = self.dec_upsample1(bottleneck)\n",
    "#         dec1 = torch.cat([dec1, enc8], dim=1)\n",
    "#         dec1 = nn.ReLU()(self.dec_conv1(dec1))\n",
    "#         dec1 = nn.ReLU()(self.dec_conv2(dec1))\n",
    "\n",
    "#         dec2 = self.dec_upsample2(dec1)\n",
    "#         dec2 = torch.cat([dec2, enc6], dim=1)\n",
    "#         dec2 = nn.ReLU()(self.dec_conv3(dec2))\n",
    "#         dec2 = nn.ReLU()(self.dec_conv4(dec2))\n",
    "\n",
    "#         dec3 = self.dec_upsample3(dec2)\n",
    "#         dec3 = torch.cat([dec3, enc4], dim=1)\n",
    "#         dec3 = nn.ReLU()(self.dec_conv5(dec3))\n",
    "#         dec3 = nn.ReLU()(self.dec_conv6(dec3))\n",
    "\n",
    "#         dec4 = self.dec_upsample4(dec3)\n",
    "#         dec4 = torch.cat([dec4, enc2], dim=1)\n",
    "#         dec4 = nn.ReLU()(self.dec_conv7(dec4))\n",
    "#         dec4 = nn.ReLU()(self.dec_conv8(dec4))\n",
    "\n",
    "#         # Output\n",
    "#         output = self.output_conv(dec4)\n",
    "#         return output   \n",
    "    \n",
    "#     # Create an instance of the FCN model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess the input image\n",
    "model = UNet()\n",
    "\n",
    "image = Image.open('R3080_realval\\drop_s30_v5_r0.5_str4_pos0_cam3.png')  # Replace 'input_image.jpg' with the path to your input image file\n",
    "\n",
    "# Create transform\n",
    "transform = transforms.Compose([\n",
    "    # transforms.Resize((256, 256)),  # Resize the image to a specific size\n",
    "    transforms.ToTensor(),    \n",
    "    transforms.Normalize(mean=[0.4414, 0.4456, 0.3421],  # Normalization of real images\n",
    "                            std=[0.2741, 0.2734, 0.3030])\n",
    "])\n",
    "\n",
    "image = image.convert('RGB')\n",
    "image = transform(image)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAOcAAADnCAYAAADl9EEgAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAALCElEQVR4nO3da3BU9R3G8efsnmQTEkgRAYWgBIgogxdEQRARq6i1giMWi3hDERWxgsLYKV471WpxingDOoy2Q6sy0hYUUbm2VoUKIkKtaLgTKoSbYEhIzF76ovQFQ4gKe/b/O5vv5x27JHmG8J2TbP678VKplADYE3E9AED9iBMwijgBo4gTMIo4AaP8hu7sHxnMQ7lAwBYkZ3r13c6VEzCKOAGjiBMwijgBo4gTMIo4AaOIEzCKOAGjiBMwijgBo4gTMIo4AaOIEzCKOAGjiBMwijgBo4gTMIo4AaOIEzCKOAGjiBMwijgBo4gTMIo4AaOIEzCKOAGjiBMwijgBo4gTMIo4AaOIEzCKOAGjiBMwijgBo4gTMIo4AaOIEzCKOAGjiBMwijgBo4gTMIo4AaOIEzCKOAGjiBMwijgBo4gTMIo4AaOIEzCKOAGjiBMwijgBo4gTMIo4AaOIEzCKOAGjiBMwijgBo4gTMIo4AaOIEzCKOAGjiBMwijgBo4gTMIo4AaOIEzCKOAGjiBMwijgBo4gTMIo4AaOIEzCKOAGjiBMwijgBo4gTMIo4AaOIEzCKOAGjiBMwijgBo4gTMIo4AaOIEzCKOAGjiBMwijgBo4gTMIo4AaOIEzCKOAGjiBMwijgBo4gTMIo4AaOIEzCKOAGjiBMwijgBo4gTMIo4AaOIEzCKOAGjiBOHiRQU6MtxvVX743NdT2nUfNcD4Fa0U4k+f/C4Q27z8+pU1neyJn3VXs8Ovlidn65WctUaRwsbLy+VSh3xzv6RwUe+E6HhxWJq9fdYvfcV5+3Vr1uvbvDt+468XfmvLwtiGiQtSM706rudK2cYRaLyIod/Pj3f1+hPVyrHix9ye1Qp9ctPHvWHe3bSc3pg9bWKb9x81O8D3x9xGuaf3E6p3JzDbh82d5GuLthT79vkeNG07zgrFtMr783Qdf2GKrFuY9rfP+pHnEbUDOihRO6hV8N7npihawv3HeEt0h9hQ4oi+Xp0wWt6YNjtiry7MqMfu7EiTsf2D+6pPV2iWjx8gk70C13PaVCPWI6GTH1b0+8fqLw5fA8aNOJ0qOonPTXiV7M0rNkOSbbD/L/hRdtV+eR8zU70V+yt5a7nZDV+zulI4qKzdf8TfzwYZriMab5JX5Ue/r0w0os4HalulauBBdWuZxy1uWMnqO6S7q5nZDXidKD2inO15Omprmcck2K/UIunv6hEv7NdT8laxJlpkajmTZvsekXa/OVPL6jqmp6uZ2Ql4sywLQ/3lJ/hH4MEqSiSr5sem+N6RlYizgx75oZpinrZ9c9+eUGZNj3ey/WMrJNd/0uM2/za6eqTV+V6Rtqd5BfqnRueUvmDvV1PySrEmUFnt92qJpFc1zMCUZJTqGV3TtTOkb2kSPZ82e4ScWZItFOJ2ubvdT0jUIWRPH380BTpnC6up2QF4syQNeNb6KkTGseZ1E0DC+X5HD47VsSZAfEfdtd13RrPWdQvbp2iskkcUDhWxJkBO8+KfesTmrPNOwMmup4QesSJQHT08+Utbut6RqgRZ8DiF3fX8rHPuJ6RcVEvormd56j8z11dTwkt4gyQl5OrHd1iinmN8xkcUS+iU1ruVLRTiespoUScAYq2aK5/3Zc952iPxuzSeVo/7ATXM0KJOBG4/pd9LPU43fWM0CHOALWaHd7na6bT820/1GUvfSC/5GTXU0KFOAM0se081xPMuO+4DUo2a+J6RqgQZ0D8Du0V8ep9reBGa/Zb0xXtcorrGaFBnAEZ/NYSFUXyXc8wJebl6OE3X3U9IzSIExnVxj+gnXfy3M/vgjgDsOXR3rowf4PrGSad5BfqvOGN4wkAx4o4A9Cmz1aV5ITjdWhdGN96oTY8ydXz2xAnMq7YL1SqXY3rGeYRZ5r5xW3VOr/S9QzzippVKdqypesZphFnmq25v1ivlPzN9QzzVnR/TZtvK3U9wzTihDPFl2xRtHMn1zPMIk44M++0N1XdqbnrGWYRJ2AUcabRnlt76dNBz7meESqzpj4jv12x6xkmEWcaJX1l7evSBqV5tIkU4QxyfYgTzm0a2s71BJOIE84tHzVJ4hk8hyHONIl2OUXX3L3Y9YxQinm+vvzraa5nmEOcaRIvytf4479wPSOUol5Ev+r6husZ5hAnYBRxAkYRJ0zokLNL+244z/UMU4gTJpyRm6fiO9a5nmEKv6ctDSIFBTpv6nLXM5BluHKmgZfj65GWn7megSxDnIBRxAkYRZyAUcQJGEWcgFHECRhFnIBRxAkYRZxpkDxQow4Lb3U9A1mGONMgVVurTlMSrmcgyxAnYBRxAkYRJ2AUcaaJv6tSt2y5wPUMZBHiTJPE2g1a/VJX1zOQRYgTMIo4YcL86hwdGJrneoYpxJlGfo20Nb7f9YxQqk7FFC/f6nqGKcSZRj+YvlR9Z491PQNZgjjhXF0qofvevt71DHOIM81KXo/rN7tLXc8IlbpUQqWjl7meYQ5xppm/aIXmV/BLeXDsiDMAsSFVmlHZ3PWM0Lh60HAplXI9wxziDEBi1269uftMJVJJ11NCwa/Y63qCScQZkJ299+rrZI3rGeaN2XaOUjW1rmeYRJxw6pOHuylRscP1DJOIM0AXThrneoJpp75/owpWb3M9wyziDFC7l9e7nmBadGVTTgU1gDgDFN9eoQtG3eF6hkmnvn+j2k1c4XqGacQZsPyKGi06EHU9w5Rt8f2KlxcoVcsDQQ0hzoB5S1bp0XG36Q9ft3I9xYSvEtU6f9Y4dRz7T9dTzCPODGgy60P9cslA1zNM+KIuptLRhPldEGeGlE6ra/RnbhOppMY8NMr1jNAgzgzxlq7Su4NO1z8a8bmEfnePVNHLXDW/K+LMoMTaDXrizAv0SSN8IGRLfL+afr7H9YxQIc4MS1ZWanz/Ia5nZNTc6jzdfPu9SqxZ63pKqBCnC5VVGrj2ctcrMuLlyhZ67KFhyn1nuespoUOcDiQqdig+okB9Vg9yPSVQ86tz9MIjg9XsVb7PPBrE6UiibL2a3ePpyrIfuZ4SiK3x/XryzpvUdAZhHi3idChRtl7JnyZ0xYWDsupBorpUQiOuvE05CzmedyyI07FExQ4l1m7QL864RB/UhP/J2f/+5oAGXHWzkqs/dz0l9IjTiGRlpR6/9Bo9taej6ylHZUXtN7ppc1/d9bPRSn30qes5WYE4DUms26gFI/qow/zhoTqLu+abag2bPEYVvb5W3hxeRS9dfNcDcChv6SqVLpVeHHS12kx4SZc2qXM9qUFdptylJttTajNtiespWcdLNfCqZ/0jg3lJNIdS55+lA61jeu/537meUq8zJ9ylE59fplQ87npKqC1IzvTqu504Q8Bv20blQ9rrvXt/q0Ivpqjn7ruRfckD6rbwbp32wDYlKnYQZhoQZ5bYOOMMPdJtrq5vujujH3d2VaHm7+2q9ec24pP7ASHOLFM7v73y/Dpd1LJMP28RzJnVN6qaaHL5RZKk6NC44tsrAvk4jR1xZqn4xd21cdD/HtdLeSmtu2rqMX3Z23HxLUrty5UktfgoouN+vzQtO3FkR4qTR2tDzl+0QqWLDv7B89Rj1Sil6v1UH/z7A3ZpWbeZ6vziSBX85/D7O8/4TIm9+wLZiu+HK2cj47crVt1Jxyu6skzJ6mrXcyCunDgoXr5VXvlWhf+gYPbjhBBgFHECRhEnYBRxAkYRJ2AUcQJGESdgFHECRhEnYBRxAkYRJ2BUgwffAbjDlRMwijgBo4gTMIo4AaOIEzCKOAGj/gtguGpLg3oqAAAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from skimage.filters import threshold_otsu\n",
    "# UNET PART\n",
    "model = UNet()\n",
    "\n",
    "# Load the trained weights\n",
    "model.load_state_dict(torch.load('Unet_R3080.pt')) # Change trained_model to use desired weights\n",
    "\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "# Make a prediction\n",
    "with torch.no_grad():\n",
    "    output = model(image.unsqueeze(0))  # Assuming you have the test image as a tensor\n",
    "\n",
    "# Convert the output tensor to a numpy array and squeeze the batch and channel dimensions\n",
    "output_1 = output.squeeze(0).squeeze(0).cpu().detach()\n",
    "output_np = output_1.numpy()\n",
    "\n",
    "threshold = 0.5  # Adjust the threshold value as needed\n",
    "# Apply a threshold to convert the output to a binary mask\n",
    "threshold_value = threshold_otsu(output_np)\n",
    "mask = (output_np >= threshold).astype(np.uint8)\n",
    "\n",
    "# Assuming tensor_image is your tensor representation of the image\n",
    "# Convert the tensor to a PIL Image\n",
    "pil_image = transforms.ToPILImage()(mask)\n",
    "\n",
    "pil_image.save(\"output_UNET.jpg\")\n",
    "\n",
    "# Convert the PIL Image to a NumPy array\n",
    "numpy_image = np.array(pil_image)\n",
    "\n",
    "# Display the image using matplotlib\n",
    "plt.imshow(numpy_image)\n",
    "plt.axis('off')  # Optional: Turn off axis labels and ticks\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for CNN:\n\tsize mismatch for conv1.weight: copying a param with shape torch.Size([16, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([16, 1, 3, 3]).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-854a67e2a842>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;31m# Load the trained weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'trained_model_Train_10200.pt'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmap_location\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'cpu'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# Change trained_model to use desired weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[1;31m# Set the model in evaluation mode\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Anaconda\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[1;34m(self, state_dict, strict)\u001b[0m\n\u001b[0;32m   1669\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1670\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1671\u001b[1;33m             raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n\u001b[0m\u001b[0;32m   1672\u001b[0m                                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n\u001b[0;32m   1673\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0m_IncompatibleKeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmissing_keys\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0munexpected_keys\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for CNN:\n\tsize mismatch for conv1.weight: copying a param with shape torch.Size([16, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([16, 1, 3, 3])."
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize(64),\n",
    "    transforms.CenterCrop(64),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.4753, 0.4753, 0.4753])\n",
    "])\n",
    "\n",
    "# Load model\n",
    "model = CNN()\n",
    "\n",
    "# Load the trained weights\n",
    "model.load_state_dict(torch.load('trained_model_Train_10200.pt',map_location=torch.device('cpu'))) # Change trained_model to use desired weights\n",
    "# Set the model in evaluation mode\n",
    "\n",
    "model.eval()\n",
    "\n",
    "input_image = pil_image.convert('RGB')\n",
    "input_tensor = transform(input_image)\n",
    "# Binarize inputs\n",
    "threshold = 0.5\n",
    "inputs = torch.where(input_tensor >= threshold, torch.ones_like(input_tensor), torch.zeros_like(input_tensor))\n",
    "inputs = inputs[:1, :, :]\n",
    "\n",
    "#Make a prediction\n",
    "with torch.no_grad():\n",
    "    output_CNN = model(inputs)\n",
    "\n",
    "# Convert the output to a readable format\n",
    "predicted_value = output_CNN.item()\n",
    "\n",
    "# Print the predicted value\n",
    "print('Predicted value:', predicted_value)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
