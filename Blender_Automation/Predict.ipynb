{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from torchvision.transforms import Normalize\n",
    "from torchvision.transforms import ToTensor\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CNN and UNET\n",
    "from CNN_Mask_to_Tens import CNN\n",
    "from UNet import UNet\n",
    "# from UNet import UNet\n",
    "# # Define the U-Net architecture for binary segmentation\n",
    "# class UNet(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(UNet, self).__init__()\n",
    "\n",
    "#         # Encoder (downsampling path)\n",
    "#         self.enc_conv1 = nn.Conv2d(3, 64, kernel_size=3, padding=1)\n",
    "#         self.enc_conv2 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
    "#         self.enc_pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "#         self.enc_conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "#         self.enc_conv4 = nn.Conv2d(128, 128, kernel_size=3, padding=1)\n",
    "#         self.enc_pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "#         self.enc_conv5 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
    "#         self.enc_conv6 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n",
    "#         self.enc_pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "#         self.enc_conv7 = nn.Conv2d(256, 512, kernel_size=3, padding=1)\n",
    "#         self.enc_conv8 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "#         self.enc_pool4 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "#         # Bottleneck\n",
    "#         self.bottleneck_conv1 = nn.Conv2d(512, 1024, kernel_size=3, padding=1)\n",
    "#         self.bottleneck_conv2 = nn.Conv2d(1024, 1024, kernel_size=3, padding=1)\n",
    "\n",
    "#         # Decoder (upsampling path)\n",
    "#         self.dec_upsample1 = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False)\n",
    "#         self.dec_conv1 = nn.Conv2d(1024 + 512, 512, kernel_size=3, padding=1)\n",
    "#         self.dec_conv2 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "\n",
    "#         self.dec_upsample2 = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False)\n",
    "#         self.dec_conv3 = nn.Conv2d(512 + 256, 256, kernel_size=3, padding=1)\n",
    "#         self.dec_conv4 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n",
    "\n",
    "#         self.dec_upsample3 = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False)\n",
    "#         self.dec_conv5 = nn.Conv2d(256 + 128, 128, kernel_size=3, padding=1)\n",
    "#         self.dec_conv6 = nn.Conv2d(128, 128, kernel_size=3, padding=1)\n",
    "\n",
    "#         self.dec_upsample4 = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False)\n",
    "#         self.dec_conv7 = nn.Conv2d(128 + 64, 64, kernel_size=3, padding=1)\n",
    "#         self.dec_conv8 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
    "\n",
    "#           # Output\n",
    "#         self.output_conv = nn.Conv2d(64, 1, kernel_size=1)  # 1x1 convolution for binary segmentation\n",
    "\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         # Encoder (downsampling path)\n",
    "#         enc1 = nn.ReLU()(self.enc_conv1(x))\n",
    "#         enc2 = nn.ReLU()(self.enc_conv2(enc1))\n",
    "#         enc2_pool = self.enc_pool1(enc2)\n",
    "\n",
    "#         enc3 = nn.ReLU()(self.enc_conv3(enc2_pool))\n",
    "#         enc4 = nn.ReLU()(self.enc_conv4(enc3))\n",
    "#         enc4_pool = self.enc_pool2(enc4)\n",
    "\n",
    "#         enc5 = nn.ReLU()(self.enc_conv5(enc4_pool))\n",
    "#         enc6 = nn.ReLU()(self.enc_conv6(enc5))\n",
    "#         enc6_pool = self.enc_pool3(enc6)\n",
    "\n",
    "#         enc7 = nn.ReLU()(self.enc_conv7(enc6_pool))\n",
    "#         enc8 = nn.ReLU()(self.enc_conv8(enc7))\n",
    "#         enc8_pool = self.enc_pool4(enc8)\n",
    "\n",
    "#         # Bottleneck\n",
    "#         bottleneck = nn.ReLU()(self.bottleneck_conv1(enc8_pool))\n",
    "#         bottleneck = nn.ReLU()(self.bottleneck_conv2(bottleneck))\n",
    "\n",
    "#         # Decoder (upsampling path)\n",
    "#         dec1 = self.dec_upsample1(bottleneck)\n",
    "#         dec1 = torch.cat([dec1, enc8], dim=1)\n",
    "#         dec1 = nn.ReLU()(self.dec_conv1(dec1))\n",
    "#         dec1 = nn.ReLU()(self.dec_conv2(dec1))\n",
    "\n",
    "#         dec2 = self.dec_upsample2(dec1)\n",
    "#         dec2 = torch.cat([dec2, enc6], dim=1)\n",
    "#         dec2 = nn.ReLU()(self.dec_conv3(dec2))\n",
    "#         dec2 = nn.ReLU()(self.dec_conv4(dec2))\n",
    "\n",
    "#         dec3 = self.dec_upsample3(dec2)\n",
    "#         dec3 = torch.cat([dec3, enc4], dim=1)\n",
    "#         dec3 = nn.ReLU()(self.dec_conv5(dec3))\n",
    "#         dec3 = nn.ReLU()(self.dec_conv6(dec3))\n",
    "\n",
    "#         dec4 = self.dec_upsample4(dec3)\n",
    "#         dec4 = torch.cat([dec4, enc2], dim=1)\n",
    "#         dec4 = nn.ReLU()(self.dec_conv7(dec4))\n",
    "#         dec4 = nn.ReLU()(self.dec_conv8(dec4))\n",
    "\n",
    "#         # Output\n",
    "#         output = self.output_conv(dec4)\n",
    "#         return output   \n",
    "    \n",
    "#     # Create an instance of the FCN model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess the input image\n",
    "model = UNet()\n",
    "\n",
    "image = Image.open('Test_Images\\Train_10200\\drop_s80_v5_r0.5_str6_pos0_cam156.png')  # Replace 'input_image.jpg' with the path to your input image file\n",
    "\n",
    "# Create transform\n",
    "transform = transforms.Compose([\n",
    "    # transforms.Resize((256, 256)),  # Resize the image to a specific size\n",
    "    transforms.ToTensor(),    \n",
    "    transforms.Normalize(mean=[0.4414, 0.4456, 0.3421],  # Normalization of real images\n",
    "                            std=[0.2741, 0.2734, 0.3030])\n",
    "])\n",
    "\n",
    "image = image.convert('RGB')\n",
    "image = transform(image)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from skimage.filters import threshold_otsu\n",
    "# UNET PART\n",
    "model = UNet()\n",
    "\n",
    "# Load the trained weights\n",
    "model.load_state_dict(torch.load('Unet_R3080.pt')) # Change trained_model to use desired weights\n",
    "\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "# Make a prediction\n",
    "with torch.no_grad():\n",
    "    output = model(image.unsqueeze(0))  # Assuming you have the test image as a tensor\n",
    "\n",
    "# Convert the output tensor to a numpy array and squeeze the batch and channel dimensions\n",
    "output_1 = output.squeeze(0).squeeze(0).cpu().detach()\n",
    "output_np = output_1.numpy()\n",
    "\n",
    "threshold = 0.5  # Adjust the threshold value as needed\n",
    "# Apply a threshold to convert the output to a binary mask\n",
    "threshold_value = threshold_otsu(output_np)\n",
    "mask = (output_np >= threshold).astype(np.uint8)\n",
    "\n",
    "# Assuming tensor_image is your tensor representation of the image\n",
    "# Convert the tensor to a PIL Image\n",
    "pil_image = transforms.ToPILImage()(mask)\n",
    "\n",
    "pil_image.save(\"output_UNET.jpg\")\n",
    "\n",
    "# Convert the PIL Image to a NumPy array\n",
    "numpy_image = np.array(pil_image)\n",
    "\n",
    "# Display the image using matplotlib\n",
    "plt.imshow(numpy_image)\n",
    "plt.axis('off')  # Optional: Turn off axis labels and ticks\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted value: 9.089672088623047\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize(64),\n",
    "    transforms.CenterCrop(64),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.4753, 0.4753, 0.4753])\n",
    "])\n",
    "\n",
    "# Load model\n",
    "model = CNN()\n",
    "\n",
    "# Load the trained weights\n",
    "model.load_state_dict(torch.load('trained_model_Train_510.pt',map_location=torch.device('cpu'))) # Change trained_model to use desired weights\n",
    "# Set the model in evaluation mode\n",
    "\n",
    "model.eval()\n",
    "\n",
    "input_image = pil_image.convert('RGB')\n",
    "input_tensor = transform(input_image)\n",
    "# Binarize inputs\n",
    "threshold = 0.5\n",
    "inputs = torch.where(input_tensor >= threshold, torch.ones_like(input_tensor), torch.zeros_like(input_tensor))\n",
    "inputs = inputs[:1, :, :]\n",
    "#Make a prediction\n",
    "with torch.no_grad():\n",
    "    output_CNN = model(inputs)\n",
    "\n",
    "# Convert the output to a readable format\n",
    "predicted_value = output_CNN.item()\n",
    "\n",
    "# Print the predicted value\n",
    "print('Predicted value:', predicted_value)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
