{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from torchvision.transforms import Normalize\n",
    "from torchvision.transforms import ToTensor\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CNN and UNET\n",
    "from CNN_Mask_to_Tens import CNN\n",
    "# from UNet import UNet\n",
    "# Define the U-Net architecture for binary segmentation\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(UNet, self).__init__()\n",
    "\n",
    "        # Encoder (downsampling path)\n",
    "        self.enc_conv1 = nn.Conv2d(3, 64, kernel_size=3, padding=1)\n",
    "        self.enc_conv2 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
    "        self.enc_pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.enc_conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.enc_conv4 = nn.Conv2d(128, 128, kernel_size=3, padding=1)\n",
    "        self.enc_pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.enc_conv5 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
    "        self.enc_conv6 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n",
    "        self.enc_pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.enc_conv7 = nn.Conv2d(256, 512, kernel_size=3, padding=1)\n",
    "        self.enc_conv8 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "        self.enc_pool4 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        # Bottleneck\n",
    "        self.bottleneck_conv1 = nn.Conv2d(512, 1024, kernel_size=3, padding=1)\n",
    "        self.bottleneck_conv2 = nn.Conv2d(1024, 1024, kernel_size=3, padding=1)\n",
    "\n",
    "        # Decoder (upsampling path)\n",
    "        self.dec_upsample1 = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False)\n",
    "        self.dec_conv1 = nn.Conv2d(1024 + 512, 512, kernel_size=3, padding=1)\n",
    "        self.dec_conv2 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "\n",
    "        self.dec_upsample2 = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False)\n",
    "        self.dec_conv3 = nn.Conv2d(512 + 256, 256, kernel_size=3, padding=1)\n",
    "        self.dec_conv4 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n",
    "\n",
    "        self.dec_upsample3 = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False)\n",
    "        self.dec_conv5 = nn.Conv2d(256 + 128, 128, kernel_size=3, padding=1)\n",
    "        self.dec_conv6 = nn.Conv2d(128, 128, kernel_size=3, padding=1)\n",
    "\n",
    "        self.dec_upsample4 = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False)\n",
    "        self.dec_conv7 = nn.Conv2d(128 + 64, 64, kernel_size=3, padding=1)\n",
    "        self.dec_conv8 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
    "\n",
    "          # Output\n",
    "        self.output_conv = nn.Conv2d(64, 1, kernel_size=1)  # 1x1 convolution for binary segmentation\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder (downsampling path)\n",
    "        enc1 = nn.ReLU()(self.enc_conv1(x))\n",
    "        enc2 = nn.ReLU()(self.enc_conv2(enc1))\n",
    "        enc2_pool = self.enc_pool1(enc2)\n",
    "\n",
    "        enc3 = nn.ReLU()(self.enc_conv3(enc2_pool))\n",
    "        enc4 = nn.ReLU()(self.enc_conv4(enc3))\n",
    "        enc4_pool = self.enc_pool2(enc4)\n",
    "\n",
    "        enc5 = nn.ReLU()(self.enc_conv5(enc4_pool))\n",
    "        enc6 = nn.ReLU()(self.enc_conv6(enc5))\n",
    "        enc6_pool = self.enc_pool3(enc6)\n",
    "\n",
    "        enc7 = nn.ReLU()(self.enc_conv7(enc6_pool))\n",
    "        enc8 = nn.ReLU()(self.enc_conv8(enc7))\n",
    "        enc8_pool = self.enc_pool4(enc8)\n",
    "\n",
    "        # Bottleneck\n",
    "        bottleneck = nn.ReLU()(self.bottleneck_conv1(enc8_pool))\n",
    "        bottleneck = nn.ReLU()(self.bottleneck_conv2(bottleneck))\n",
    "\n",
    "        # Decoder (upsampling path)\n",
    "        dec1 = self.dec_upsample1(bottleneck)\n",
    "        dec1 = torch.cat([dec1, enc8], dim=1)\n",
    "        dec1 = nn.ReLU()(self.dec_conv1(dec1))\n",
    "        dec1 = nn.ReLU()(self.dec_conv2(dec1))\n",
    "\n",
    "        dec2 = self.dec_upsample2(dec1)\n",
    "        dec2 = torch.cat([dec2, enc6], dim=1)\n",
    "        dec2 = nn.ReLU()(self.dec_conv3(dec2))\n",
    "        dec2 = nn.ReLU()(self.dec_conv4(dec2))\n",
    "\n",
    "        dec3 = self.dec_upsample3(dec2)\n",
    "        dec3 = torch.cat([dec3, enc4], dim=1)\n",
    "        dec3 = nn.ReLU()(self.dec_conv5(dec3))\n",
    "        dec3 = nn.ReLU()(self.dec_conv6(dec3))\n",
    "\n",
    "        dec4 = self.dec_upsample4(dec3)\n",
    "        dec4 = torch.cat([dec4, enc2], dim=1)\n",
    "        dec4 = nn.ReLU()(self.dec_conv7(dec4))\n",
    "        dec4 = nn.ReLU()(self.dec_conv8(dec4))\n",
    "\n",
    "        # Output\n",
    "        output = self.output_conv(dec4)\n",
    "        return output   \n",
    "    \n",
    "    # Create an instance of the FCN model\n",
    "model = UNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess the input image\n",
    "image = Image.open('Data/R3080_realval/drop_s30_v5_r0.5_str4_pos0_cam0.png')  # Replace 'input_image.jpg' with the path to your input image file\n",
    "\n",
    "# Create transform\n",
    "transform = transforms.Compose([\n",
    "    # transforms.Resize((256, 256)),  # Resize the image to a specific size\n",
    "    transforms.ToTensor(),    \n",
    "    transforms.Normalize(mean=[0.4414, 0.4456, 0.3421],  # Normalization of real images\n",
    "                            std=[0.2741, 0.2734, 0.3030])\n",
    "])\n",
    "\n",
    "image = image.convert('RGB')\n",
    "image = transform(image)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from skimage.filters import threshold_otsu\n",
    "# UNET PART\n",
    "model = UNet()\n",
    "\n",
    "# Load the trained weights\n",
    "model.load_state_dict(torch.load('Unet_R3080.pt')) # Change trained_model to use desired weights\n",
    "\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "# Make a prediction\n",
    "with torch.no_grad():\n",
    "    output = model(image.unsqueeze(0))  # Assuming you have the test image as a tensor\n",
    "\n",
    "# Convert the output tensor to a numpy array and squeeze the batch and channel dimensions\n",
    "output_1 = output.squeeze(0).squeeze(0).cpu().detach()\n",
    "output_np = output_1.numpy()\n",
    "\n",
    "threshold = 0.5  # Adjust the threshold value as needed\n",
    "# Apply a threshold to convert the output to a binary mask\n",
    "threshold_value = threshold_otsu(output_np)\n",
    "mask = (output_np >= threshold).astype(np.uint8)\n",
    "\n",
    "# Assuming tensor_image is your tensor representation of the image\n",
    "# Convert the tensor to a PIL Image\n",
    "pil_image = transforms.ToPILImage()(mask)\n",
    "\n",
    "pil_image.save(\"output_UNET.jpg\")\n",
    "\n",
    "# Convert the PIL Image to a NumPy array\n",
    "numpy_image = np.array(pil_image)\n",
    "\n",
    "# Display the image using matplotlib\n",
    "plt.imshow(numpy_image)\n",
    "plt.axis('off')  # Optional: Turn off axis labels and ticks\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted value: 28.731781005859375\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize(64),\n",
    "    transforms.CenterCrop(64),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.4753, 0.4753, 0.4753])\n",
    "])\n",
    "\n",
    "# Load model\n",
    "model = CNN()\n",
    "\n",
    "# Load the trained weights\n",
    "model.load_state_dict(torch.load('trained_model_Train_510.pt',map_location=torch.device('cpu'))) # Change trained_model to use desired weights\n",
    "# Set the model in evaluation mode\n",
    "\n",
    "model.eval()\n",
    "\n",
    "input_image = pil_image.convert('RGB')\n",
    "input_tensor = transform(input_image)\n",
    "\n",
    "# Make a prediction\n",
    "with torch.no_grad():\n",
    "    output_CNN = model(input_tensor)\n",
    "\n",
    "# Convert the output to a readable format\n",
    "predicted_value = output_CNN.item()\n",
    "\n",
    "# Print the predicted value\n",
    "print('Predicted value:', predicted_value)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
