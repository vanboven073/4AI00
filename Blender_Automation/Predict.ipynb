{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from torchvision.transforms import Normalize\n",
    "from torchvision.transforms import ToTensor\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CNN and UNET\n",
    "from CNN_Mask_to_Tens import CNN\n",
    "from UNet import UNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess the input image\n",
    "image = Image.open('Test_Images\\Train_10200\\drop_s30_v5_r0.5_str6_pos0_cam1.png')  # Replace 'input_image.jpg' with the path to your input image file\n",
    "\n",
    "# Create transform\n",
    "transform = transforms.Compose([\n",
    "    # transforms.Resize((256, 256)),  # Resize the image to a specific size\n",
    "    transforms.ToTensor(),    \n",
    "    transforms.Normalize(mean=[0.8451, 0.8839, 0.9087],  # Simple, small angle\n",
    "                            std=[0.1665, 0.0732, 0.0229])\n",
    "    # (mean=[0.8463, 0.8872, 0.9100],\n",
    "    #                         std=[0.1631, 0.0693, 0.0253])\n",
    "])\n",
    "\n",
    "image = image.convert('RGB')\n",
    "image = transform(image)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# UNET PART\n",
    "model = UNet()\n",
    "\n",
    "# Load the trained weights\n",
    "model.load_state_dict(torch.load('Unet_R3080.pt')) # Change trained_model to use desired weights\n",
    "\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "# Make a prediction\n",
    "with torch.no_grad():\n",
    "    output = model(image.unsqueeze(0))  # Assuming you have the test image as a tensor\n",
    "\n",
    "# Convert the output tensor to a numpy array and squeeze the batch and channel dimensions\n",
    "output_1 = output.squeeze(0).squeeze(0).cpu().detach()\n",
    "output_np = output_1.numpy()\n",
    "\n",
    "threshold = 0.5  # Adjust the threshold value as needed\n",
    "# Apply a threshold to convert the output to a binary mask\n",
    "mask = (output_np >= threshold).astype(np.uint8)\n",
    "\n",
    "# Assuming tensor_image is your tensor representation of the image\n",
    "# Convert the tensor to a PIL Image\n",
    "pil_image = transforms.ToPILImage()(mask)\n",
    "\n",
    "pil_image.save(\"output_UNET.jpg\")\n",
    "\n",
    "# Convert the PIL Image to a NumPy array\n",
    "numpy_image = np.array(pil_image)\n",
    "\n",
    "# Display the image using matplotlib\n",
    "plt.imshow(numpy_image)\n",
    "plt.axis('off')  # Optional: Turn off axis labels and ticks\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted value: 51.47602844238281\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize(64),\n",
    "    transforms.CenterCrop(64),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.4753, 0.4753, 0.4753])\n",
    "])\n",
    "\n",
    "# Load model\n",
    "model = CNN()\n",
    "\n",
    "# Load the trained weights\n",
    "model.load_state_dict(torch.load('trained_model_Train_510.pt')) # Change trained_model to use desired weights\n",
    "# Set the model in evaluation mode\n",
    "\n",
    "model.eval()\n",
    "\n",
    "input_image = pil_image.convert('RGB')\n",
    "input_tensor = transform(input_image)\n",
    "\n",
    "# Make a prediction\n",
    "with torch.no_grad():\n",
    "    output_CNN = model(input_tensor)\n",
    "\n",
    "# Convert the output to a readable format\n",
    "predicted_value = output_CNN.item()\n",
    "\n",
    "# Print the predicted value\n",
    "print('Predicted value:', predicted_value)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
