{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms, datasets\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import torchvision\n",
    "import glob\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torchvision.transforms import Normalize\n",
    "from torchvision.transforms import ToTensor\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the device for computation (CPU or GPU)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module): # results in accuracy of 73.22%\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.conv4 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.conv5 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.fc1 = nn.Linear(256 * 2 * 2, 1024)\n",
    "        self.fc2 = nn.Linear(1024, 512)\n",
    "        self.fc3 = nn.Linear(512, 256)\n",
    "        self.fc4 = nn.Linear(256, 128)\n",
    "        self.fc5 = nn.Linear(128, 1)\n",
    "        self.leaky_relu = nn.LeakyReLU(negative_slope=0.2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(self.leaky_relu(self.conv1(x)))\n",
    "        x = self.pool(self.leaky_relu(self.conv2(x)))\n",
    "        x = self.pool(self.leaky_relu(self.conv3(x)))\n",
    "        x = self.pool(self.leaky_relu(self.conv4(x)))\n",
    "        x = self.pool(self.leaky_relu(self.conv5(x)))\n",
    "        x = x.view(-1, 256 * 2 * 2)\n",
    "        x = self.leaky_relu(self.fc1(x))\n",
    "        x = self.leaky_relu(self.fc2(x))\n",
    "        x = self.leaky_relu(self.fc3(x))\n",
    "        x = self.leaky_relu(self.fc4(x))\n",
    "        x = self.fc5(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class CNN(nn.Module): # OVERFITS\n",
    "#     def __init__(self):\n",
    "#         super(CNN, self).__init__()\n",
    "#         self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1)\n",
    "#         self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
    "#         self.conv3 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "#         self.conv4 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "#         self.conv5 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
    "#         self.conv6 = nn.Conv2d(256, 512, kernel_size=3, padding=1)\n",
    "#         self.conv7 = nn.Conv2d(512, 1024, kernel_size=3, padding=1)  # Additional layer\n",
    "#         self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "#         self.fc1 = nn.Linear(1024 * 2 * 2, 1024)  # Updated input size\n",
    "#         self.fc2 = nn.Linear(1024, 512)\n",
    "#         self.fc3 = nn.Linear(512, 256)\n",
    "#         self.fc4 = nn.Linear(256, 128)\n",
    "#         self.fc5 = nn.Linear(128, 64)  # New layer with 64 output neurons\n",
    "#         self.fc6 = nn.Linear(64, 1)  # Additional layer with 1 output neuron\n",
    "#         self.leaky_relu = nn.LeakyReLU(negative_slope=0.2)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.pool(self.leaky_relu(self.conv1(x)))\n",
    "#         x = self.pool(self.leaky_relu(self.conv2(x)))\n",
    "#         x = self.pool(self.leaky_relu(self.conv3(x)))\n",
    "#         x = self.pool(self.leaky_relu(self.conv4(x)))\n",
    "#         x = self.pool(self.leaky_relu(self.conv5(x)))\n",
    "#         x = self.pool(self.leaky_relu(self.conv6(x)))\n",
    "#         x = self.leaky_relu(self.conv7(x))  # Additional layer\n",
    "#         x = x.view(-1, 1024 * 2 * 2)  # Reshape the tensor\n",
    "#         x = self.leaky_relu(self.fc1(x))\n",
    "#         x = self.leaky_relu(self.fc2(x))\n",
    "#         x = self.leaky_relu(self.fc3(x))\n",
    "#         x = self.leaky_relu(self.fc4(x))\n",
    "#         x = self.leaky_relu(self.fc5(x))  # Apply activation to new layer\n",
    "#         x = self.fc6(x)  # Pass through additional layer\n",
    "#         return x.squeeze(1)  # Squeeze the output tensor\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class CNN(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(CNN, self).__init__()\n",
    "#         self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
    "#         self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "#         self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "#         self.conv4 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
    "#         self.conv5 = nn.Conv2d(256, 512, kernel_size=3, padding=1)\n",
    "#         self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "#         self.fc1 = nn.Linear(512 * 2 * 2, 2048)\n",
    "#         self.fc2 = nn.Linear(2048, 1024)\n",
    "#         self.fc3 = nn.Linear(1024, 512)\n",
    "#         self.fc4 = nn.Linear(512, 256)\n",
    "#         self.fc5 = nn.Linear(256, 1)\n",
    "#         self.leaky_relu = nn.LeakyReLU(negative_slope=0.2)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.pool(self.leaky_relu(self.conv1(x)))\n",
    "#         x = self.pool(self.leaky_relu(self.conv2(x)))\n",
    "#         x = self.pool(self.leaky_relu(self.conv3(x)))\n",
    "#         x = self.pool(self.leaky_relu(self.conv4(x)))\n",
    "#         x = self.pool(self.leaky_relu(self.conv5(x)))\n",
    "#         x = x.view(-1, 512 * 2 * 2)\n",
    "#         x = self.leaky_relu(self.fc1(x))\n",
    "#         x = self.leaky_relu(self.fc2(x))\n",
    "#         x = self.leaky_relu(self.fc3(x))\n",
    "#         x = self.leaky_relu(self.fc4(x))\n",
    "#         x = self.fc5(x)\n",
    "#         return x.squeeze(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Training Loss: 2648.324261 - Validation Loss: 657.574555 - Learning Rate: 0.001\n",
      "Epoch 1 completed.\n",
      "Epoch 2 - Training Loss: 719.099103 - Validation Loss: 609.254205 - Learning Rate: 0.001\n",
      "Epoch 2 completed.\n",
      "Epoch 3 - Training Loss: 548.199274 - Validation Loss: 453.330608 - Learning Rate: 0.001\n",
      "Epoch 3 completed.\n",
      "Epoch 4 - Training Loss: 504.690854 - Validation Loss: 303.249300 - Learning Rate: 0.001\n",
      "Epoch 4 completed.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-224-8e41bbf65813>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    107\u001b[0m             \u001b[0minputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# Move inputs to the device\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    108\u001b[0m             \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# Move labels to the device\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 109\u001b[1;33m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    110\u001b[0m             \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    111\u001b[0m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Anaconda\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1195\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-221-2921596abb8d>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpool\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mleaky_relu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpool\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mleaky_relu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv3\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpool\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mleaky_relu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv4\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpool\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mleaky_relu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv5\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m256\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m2\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Anaconda\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1195\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Anaconda\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    461\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    462\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 463\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    464\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    465\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Anaconda\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    457\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    458\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m--> 459\u001b[1;33m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0m\u001b[0;32m    460\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[0;32m    461\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Create training dataset\n",
    "# Set up the data directories\n",
    "sets = ['Train_510', 'Train_1020', 'Train_2040', 'Train_4080', 'Train_8160', 'Train_10200']\n",
    "best_train_losses = {}  # Dictionary to store the best training loss for each item\n",
    "best_val_losses = {}  # Dictionary to store the best validation loss for each item\n",
    "accuracy = []\n",
    "for item in sets:\n",
    "    data_dir = 'Test_Images'\n",
    "    train_dir = os.path.join(data_dir, item)\n",
    "\n",
    "    # Define a function to get the labels from the image filenames\n",
    "    def get_label(filename):\n",
    "        match = re.search(r'\\d+\\.?\\d*', filename)\n",
    "        if match:\n",
    "            return float(match.group())\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    # Define a list to store the image filenames and labels\n",
    "    train_data = []\n",
    "\n",
    "    # Iterate over the training images and add them to the list\n",
    "    for filename in os.listdir(train_dir):\n",
    "        label = get_label(filename)\n",
    "        if label is not None:\n",
    "            train_data.append([os.path.join(train_dir, filename), label])\n",
    "\n",
    "    # Convert the list to a dataframe\n",
    "    train_df = pd.DataFrame(train_data, columns=['filename', 'label'])\n",
    "\n",
    "    # Save the dataframe to a CSV file\n",
    "    train_df.to_csv(os.path.join(data_dir, item+'.csv'), index=False)\n",
    "\n",
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, csv_file, image_folder, transform=None):\n",
    "        self.data = self._load_data(csv_file)\n",
    "        self.image_folder = image_folder\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image_path, label = self.data[index]\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label\n",
    "\n",
    "    def _load_data(self, csv_file):\n",
    "        data = []\n",
    "        with open(csv_file, 'r') as file:\n",
    "            lines = file.readlines()\n",
    "            for line in lines[1:]:\n",
    "                image_path, label = line.strip().split(',')\n",
    "                data.append((image_path, float(label)))\n",
    "        return data\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(64),\n",
    "    transforms.CenterCrop(64),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.4753, 0.4753, 0.4753])\n",
    "])\n",
    "\n",
    "for item in sets:\n",
    "    csv_file = f'Test_Images/{item}.csv'\n",
    "    image_folder = f'Test_Images/{item}'\n",
    "\n",
    "    dataset = CustomImageDataset(csv_file, image_folder, transform=transform)\n",
    "\n",
    "    train_size = int(0.6 * len(dataset))\n",
    "    val_size = int(0.2 * len(dataset))\n",
    "    test_size = len(dataset) - train_size - val_size\n",
    "\n",
    "    train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "    num_epochs = 5\n",
    "    best_val_loss = float('inf')\n",
    "    best_learning_rate = None\n",
    "    patience = 5\n",
    "    counter = 0\n",
    "    learning_rate = 0.001\n",
    "\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    epoch_numbers = []\n",
    "    item_train_losses = []  # List to store training losses for the current item\n",
    "    item_val_losses = []  # List to store validation losses for the current item\n",
    "\n",
    "    model = CNN().to(device)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "\n",
    "        model.train()\n",
    "        for i, (inputs, labels) in enumerate(train_loader, 0):\n",
    "            optimizer.zero_grad()\n",
    "            inputs = inputs.to(device)  # Move inputs to the device\n",
    "            labels = labels.to(device)  # Move labels to the device\n",
    "            outputs = model(inputs)\n",
    "            labels = labels.unsqueeze(1)\n",
    "            loss = criterion(outputs, labels.float())\n",
    "\n",
    "            l1_lambda = 0.01\n",
    "            l1_regularization = torch.tensor(0.).to(device)\n",
    "            for param in model.parameters():\n",
    "                l1_regularization += torch.norm(param, 1).to(device)\n",
    "            loss += l1_lambda * l1_regularization\n",
    "\n",
    "            l2_lambda = 0.01\n",
    "            l2_regularization = torch.tensor(0.).to(device)\n",
    "            for param in model.parameters():\n",
    "                l2_regularization += torch.norm(param, 2).to(device)\n",
    "            loss += l2_lambda * l2_regularization\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if i % 100 == 99:\n",
    "                print('[%d, %5d] loss: %.6f' % (epoch + 1, i + 1, running_loss / 100))\n",
    "                running_loss = 0.0\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_loss = 0.0\n",
    "            val_samples = 0\n",
    "            for inputs, labels in val_loader:\n",
    "                outputs = model(inputs)\n",
    "                labels = labels.unsqueeze(1)\n",
    "                loss = criterion(outputs, labels.float())\n",
    "                val_loss += loss.item() * inputs.size(0)\n",
    "                val_samples += inputs.size(0)\n",
    "\n",
    "            average_val_loss = val_loss / val_samples\n",
    "\n",
    "            train_losses.append(running_loss / len(train_loader))\n",
    "            val_losses.append(average_val_loss)\n",
    "            item_train_losses.append(running_loss / len(train_loader))\n",
    "            item_val_losses.append(average_val_loss)\n",
    "\n",
    "            epoch_numbers.append(epoch + 1)\n",
    "            print('Epoch %d - Training Loss: %.6f - Validation Loss: %.6f - Learning Rate: %.3f' % (epoch + 1, running_loss / len(train_loader), average_val_loss, learning_rate))\n",
    "\n",
    "        if average_val_loss < best_val_loss:\n",
    "            best_val_loss = average_val_loss\n",
    "            counter = 0\n",
    "            torch.save(model.state_dict(), f'trained_model_{item}.pt')\n",
    "\n",
    "        else:\n",
    "            counter += 1\n",
    "            if counter >= patience:\n",
    "                print('No improvement in validation loss. Early stopping.')\n",
    "                break\n",
    "\n",
    "        print('Epoch %d completed.' % (epoch + 1))\n",
    "\n",
    "    best_train_loss = min(item_train_losses)\n",
    "    best_train_losses[item] = best_train_loss\n",
    "\n",
    "    best_val_loss = min(item_val_losses)\n",
    "    best_val_losses[item] = best_val_loss\n",
    "    # Plotting the losses\n",
    "    # epochs = range(1, num_epochs + 1)\n",
    "    plt.plot(epoch_numbers, train_losses, label='Training Loss')\n",
    "    plt.plot(epoch_numbers, val_losses, label='Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.legend()\n",
    "    plt.savefig(f'loss_plot_{item}.jpeg', format='jpeg')\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "   \n",
    "    # # Create a DataFrame to store the predictions\n",
    "    # predictions_df = pd.DataFrame(columns=['Image', 'Real Value', 'Predicted Value', 'Difference'])\n",
    "\n",
    "    # # Iterate over the figures in the folder\n",
    "    # for filename in os.listdir(image_folder):\n",
    "    #     if filename.endswith('.jpg') or filename.endswith('.png'):  # Adjust the file extensions as per your figures\n",
    "    #         image_path = os.path.join(image_folder, filename)\n",
    "            \n",
    "    #         # Load and preprocess the image\n",
    "    #         image = Image.open(image_path).convert('RGB')\n",
    "    #         input_tensor = transform(image).unsqueeze(0).to(device)\n",
    "            \n",
    "    #         # Extract the real value from the filename\n",
    "    #         real_value = int(filename.split('_')[1][1:])  # Adjust the splitting pattern to extract the desired value\n",
    "            \n",
    "    #         # Make the prediction\n",
    "    #         with torch.no_grad():\n",
    "    #             output = model(input_tensor)\n",
    "    #         predicted_value = output.item()\n",
    "            \n",
    "    #         # Calculate the difference between predicted and real value\n",
    "    #         difference = predicted_value - real_value\n",
    "\n",
    "    #         # Add the prediction, real value, and difference to the DataFrame\n",
    "    #         predictions_df = predictions_df.append({'Image': filename, 'Real Value': real_value, 'Predicted Value': predicted_value, 'Difference': difference}, ignore_index=True)\n",
    "\n",
    "    #     # # Print the predictions table\n",
    "    #     # print(predictions_df)\n",
    "\n",
    "    # Create a DataFrame to store the predictions\n",
    "    predictions_test_df = pd.DataFrame(columns=['Real Value', 'Predicted Value', 'Difference'])\n",
    "\n",
    "    model = CNN().to(device)  # Instantiate a new instance of the CNN class\n",
    "    model.load_state_dict(torch.load(f'trained_model_{item}.pt'))  # Load the state dictionary of the pre-trained model\n",
    "\n",
    "    # Set the model to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # Initialize variables for tracking correct predictions and total samples\n",
    "    correct_predictions = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    # Define the range for considering a prediction as correct\n",
    "    max_range = 0.5\n",
    "\n",
    "    # Iterate over the test data\n",
    "    for sample in test_loader:\n",
    "\n",
    "        # Move the input data to the device\n",
    "        inputs = sample[0].to(device)  # Assuming the input images are the first element in each sample\n",
    "        labels = sample[1].to(device)  # Assuming the labels are the second element in each sample\n",
    "\n",
    "        # Forward pass through the model\n",
    "        with torch.no_grad():\n",
    "            predicted_values = model(inputs).squeeze().tolist()\n",
    "\n",
    "        # Get the predicted labels\n",
    "        predicted_labels = outputs  # Assuming the model output is a single scalar value\n",
    "\n",
    "        # Calculate the number of correct predictions within the range\n",
    "        # correct_predictions += ((predicted_labels >= min_range) & (predicted_labels <= max_range) & (labels >= min_range) & (labels <= max_range)).sum().item()\n",
    "        \n",
    "    # Iterate over the predicted values and add them to the DataFrame\n",
    "        for i in range(len(predicted_values)):\n",
    "            real_value = labels[i].item()\n",
    "            predicted_value = predicted_values[i]\n",
    "\n",
    "            # Check if the predicted value is within the desired range (40-70)\n",
    "            if real_value >= 40 and real_value <= 70:\n",
    "                # Calculate the difference between predicted and real value\n",
    "                difference = predicted_value - real_value\n",
    "\n",
    "                # Add the prediction, real value, and difference to the DataFrame\n",
    "                predictions_test_df = predictions_test_df.append({'Real Value': real_value, 'Predicted Value': predicted_value, 'Difference': difference}, ignore_index=True)\n",
    "\n",
    "                # Check if the prediction is correct within the desired range\n",
    "                if abs(real_value - predicted_value) <= max_range:\n",
    "                    correct_predictions += 1\n",
    "\n",
    "\n",
    "    # Calculate the accuracy\n",
    "    accuracy_value = correct_predictions / (len(predictions_test_df)-1)\n",
    "    accuracy.append(accuracy_value)\n",
    "\n",
    "    # # Print the predictions DataFrame\n",
    "    # print(predictions_test_df)\n",
    "\n",
    "    # Print the accuracy\n",
    "    print(f\"Accuracy for {item}: {accuracy_value * 100:.2f}%\")\n",
    "    \n",
    "for item in sets:\n",
    "    print(f\"Item: {item}\")\n",
    "    print(f\"Best Validation Loss: {best_val_losses[item]}\")\n",
    "    print(f\"Best Training Loss: {best_train_losses[item]}\")\n",
    "    print(f\"Accuracy for {item}: {accuracy_value * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Item: Train_510\n",
      "Best Validation Loss: 213.3171792123832\n",
      "Best Training Loss: 344.830419921875\n",
      "Accuracy for Train_510: 0.00%\n",
      "Item: Train_1020\n",
      "Best Validation Loss: 2.5618433928957174\n",
      "Best Training Loss: 65.54328365325928\n",
      "Accuracy for Train_1020: 3.15%\n",
      "Item: Train_2040\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'Train_2040'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-119-9cca8083b397>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mitem\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msets\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Item: {item}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Best Validation Loss: {best_val_losses[item]}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Best Training Loss: {best_train_losses[item]}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Accuracy for {item}: {accuracy[i] * 100:.2f}%\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Train_2040'"
     ]
    }
   ],
   "source": [
    "for i, item in enumerate(sets):\n",
    "    print(f\"Item: {item}\")\n",
    "    print(f\"Best Validation Loss: {best_val_losses[item]}\")\n",
    "    print(f\"Best Training Loss: {best_train_losses[item]}\")\n",
    "    print(f\"Accuracy for {item}: {accuracy[i] * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "x and y must be the same size",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-120-2026af660b89>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# sets = ['Train_510', 'Train_1020', 'Train_2040', 'Train_4080', 'Train_8160', 'Train_10200']\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscatter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Data set size'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Accuracy'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Anaconda\\lib\\site-packages\\matplotlib\\pyplot.py\u001b[0m in \u001b[0;36mscatter\u001b[1;34m(x, y, s, c, marker, cmap, norm, vmin, vmax, alpha, linewidths, verts, edgecolors, plotnonfinite, data, **kwargs)\u001b[0m\n\u001b[0;32m   2888\u001b[0m         \u001b[0mverts\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcbook\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdeprecation\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_deprecated_parameter\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2889\u001b[0m         edgecolors=None, *, plotnonfinite=False, data=None, **kwargs):\n\u001b[1;32m-> 2890\u001b[1;33m     __ret = gca().scatter(\n\u001b[0m\u001b[0;32m   2891\u001b[0m         \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mc\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmarker\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmarker\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcmap\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcmap\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnorm\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnorm\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2892\u001b[0m         \u001b[0mvmin\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvmin\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvmax\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvmax\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0malpha\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlinewidths\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlinewidths\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Anaconda\\lib\\site-packages\\matplotlib\\__init__.py\u001b[0m in \u001b[0;36minner\u001b[1;34m(ax, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1445\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0minner\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0max\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1446\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1447\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0max\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msanitize_sequence\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1448\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1449\u001b[0m         \u001b[0mbound\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnew_sig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0max\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Anaconda\\lib\\site-packages\\matplotlib\\cbook\\deprecation.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*inner_args, **inner_kwargs)\u001b[0m\n\u001b[0;32m    409\u001b[0m                          \u001b[1;32melse\u001b[0m \u001b[0mdeprecation_addendum\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    410\u001b[0m                 **kwargs)\n\u001b[1;32m--> 411\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minner_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0minner_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    412\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    413\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Anaconda\\lib\\site-packages\\matplotlib\\axes\\_axes.py\u001b[0m in \u001b[0;36mscatter\u001b[1;34m(self, x, y, s, c, marker, cmap, norm, vmin, vmax, alpha, linewidths, verts, edgecolors, plotnonfinite, **kwargs)\u001b[0m\n\u001b[0;32m   4439\u001b[0m         \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4440\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4441\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"x and y must be the same size\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4442\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4443\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0ms\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: x and y must be the same size"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(sets, accuracy)\n",
    "plt.xlabel('Data set size')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Accuracy for Different Sets')\n",
    "plt.xticks(rotation=45)  # Rotate the x-axis labels if needed\n",
    "plt.savefig('Accuracy per data set.jpeg', format='jpeg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Train_2040'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-176-3d025d43b9f1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Prepare data for scatter plot\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mx_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msets\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0my_val_losses\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mbest_val_losses\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msets\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0my_train_losses\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mbest_train_losses\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msets\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-176-3d025d43b9f1>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Prepare data for scatter plot\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mx_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msets\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0my_val_losses\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mbest_val_losses\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msets\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0my_train_losses\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mbest_train_losses\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msets\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Train_2040'"
     ]
    }
   ],
   "source": [
    "# Prepare data for scatter plot\n",
    "x_values = sets\n",
    "y_val_losses = [best_val_losses[item] for item in sets]\n",
    "y_train_losses = [best_train_losses[item] for item in sets]\n",
    "\n",
    "# Create scatter plot\n",
    "plt.scatter(x_values, y_val_losses, label='Best Validation Loss')\n",
    "plt.scatter(x_values, y_train_losses, label='Best Training Loss')\n",
    "\n",
    "# Set labels and title\n",
    "plt.xlabel('Sets')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Best Validation and Training Loss per set')\n",
    "\n",
    "# Add legend\n",
    "plt.legend()\n",
    "\n",
    "# Save the plot\n",
    "plt.savefig('Best_losses_per_set.jpeg', format='jpeg')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
